# üöÄ Construindo Pipelines de Dados Modernos: ETL/ELT com Apache Airflow 3

## üìë Introdu√ß√£o

Bem-vindo √† aula pr√°tica focada em **Automa√ß√£o e Orquestra√ß√£o de Fluxos de Dados** utilizando ferramentas modernas de Engenharia de Dados. Neste laborat√≥rio de 3 horas, transformaremos dados brutos de vendas em informa√ß√µes estruturadas, criando um pipeline completo com Apache Airflow 3 e PostgreSQL.

### üéØ Objetivo da Aula

* Compreender os conceitos de pipelines de dados automatizados e as abordagens **ETL** e **ELT**.
* Ganhar experi√™ncia pr√°tica desenvolvendo uma **DAG (Directed Acyclic Graph)** completa para processamento de dados.
* Dominar o uso de `PythonOperator` e `PostgresOperator` no Apache Airflow.

---

## üèóÔ∏è Arquitetura do Ambiente (Docker Compose)

Nosso ambiente √© totalmente conteinerizado, garantindo que todos os participantes usem o mesmo setup.

| Componente | Fun√ß√£o | Detalhes de Acesso |
| :--- | :--- | :--- |
| **Airflow 3 Standalone** | Orquestra√ß√£o (Webserver, Scheduler, Executor) | URL: `http://localhost:8080` |
| **PostgreSQL** | Base de Dados de Destino (`northwind` DB) | Host: `postgres_erp` (interno), Porta Externa: `2001` |

### Mapeamento de Volumes

| Diret√≥rio Local | Diret√≥rio no Container | Prop√≥sito |
| :--- | :--- | :--- |
| `./dags` | `/opt/airflow/dags` | Armazenamento dos arquivos `.py` da DAG. |
| `./data` | `/opt/airflow/data` | Fonte de dados (Ex: `dados_vendas.csv`). |
| `./logs` | `/opt/airflow/logs` | Logs de execu√ß√£o do Airflow. |

### Fluxo do Pipeline ETL

O pipeline simula um fluxo b√°sico, indo do arquivo CSV para a tabela final no banco de dados.
CSV File ‚îÄ‚îÄ‚ñ∫ Extract ‚îÄ‚îÄ‚ñ∫ Transform ‚îÄ‚îÄ‚ñ∫ Load ‚îÄ‚îÄ‚ñ∫ PostgreSQL (dados_vendas.csv)


---

## üìñ Estrutura da Aula

### 1. Conceitos Fundamentais
* **O que s√£o Pipelines de Dados?** Defini√ß√£o, prop√≥sito e componentes (ingest√£o, transforma√ß√£o, orquestra√ß√£o).
* **Abordagens de Transforma√ß√£o:**
    * **ETL (Extract, Transform, Load):** Foco em transformar em ambiente *staging* antes de carregar.
    * **ELT (Extract, Load, Transform):** Foco em carregar dados brutos primeiro e usar o poder do destino para transformar.
* **Compara√ß√£o e Escolha:** Vantagens, desvantagens e fatores decisivos (Volume de Dados, Complexidade da Transforma√ß√£o, Capacidade do Destino).

### 2. Ferramentas no Ecossistema de Dados
* **Orquestra√ß√£o:** Apache Airflow (DAGs, Scheduler, UI).
* **Transforma√ß√£o:** Pandas (manipula√ß√£o em mem√≥ria), DBT (transforma√ß√µes SQL), Spark (processamento distribu√≠do).
* **Ingest√£o e Armazenamento:** Apache Kafka, Airbyte, Data Warehouses (Ex: Snowflake, BigQuery).

### 3. Laborat√≥rio Pr√°tico: Airflow Hands-on
* **Setup do Ambiente:** Inicializa√ß√£o do Docker e acesso ao Airflow UI.
* **Desenvolvimento da DAG:** Cria√ß√£o da DAG `etl_vendas_pipeline`.
* **Implementa√ß√£o das Tasks:**
    1.  `extract_data`: Leitura e valida√ß√£o do CSV.
    2.  `transform_data`: Limpeza de nulos e c√°lculo de m√©tricas (`Valor * Quantidade`).
    3.  `load_data`: Cria√ß√£o da tabela e inser√ß√£o dos dados no PostgreSQL.

---

## üíª Setup do Ambiente (Passos Pr√°ticos)

Para iniciar o laborat√≥rio, siga os passos abaixo no seu terminal:

### Pr√©-requisitos
* Docker e Docker Compose instalados.
* Python 3.8+ (para desenvolvimento local de DAGs).

### Inicializa√ß√£o
1.  **Build da Imagem (Primeira vez):**
    ```bash
    docker compose build
    ```

2.  **Inicializa√ß√£o e Setup do Airflow (Inicializa√ß√£o de DB e Usu√°rio):**
    ```bash
    docker compose up --no-deps --wait airflow-init
    ```

3.  **Subir os Servi√ßos (Airflow e PostgreSQL):**
    ```bash
    docker compose up -d
    ```

### Acesso e Credenciais
* **Airflow UI:** `http://localhost:8080`
* **Usu√°rio/Senha:** `admin` / `[airflow_token]` (Verifique o token no log de inicializa√ß√£o se o `airflow-init` n√£o o definir automaticamente).

---

## üß™ Laborat√≥rio Pr√°tico: Cen√°rio e Dados

### Cen√°rio ETL

Criaremos o pipeline ETL para processar dados de vendas do arquivo `data/dados_vendas.csv` e carregar na tabela `vendas` do PostgreSQL.

### Estrutura dos Dados (`dados_vendas.csv`)

| Campo | Descri√ß√£o |
| :--- | :--- |
| `ID_Produto` | Identificador do Produto |
| `Valor` | Pre√ßo unit√°rio da venda |
| `Quantidade` | Quantidade vendida na transa√ß√£o |
| `Data` | Data da transa√ß√£o |
| `Regiao` | Regi√£o de origem da venda |

### Comandos √öteis

| Comando | Descri√ß√£o |
| :--- | :--- |
| **Ver Logs do Airflow:** | `docker compose logs -f airflow-standalone` |
| **Parar Servi√ßos:** | `docker compose down` |
| **Acessar Container do Airflow:** | `docker compose exec airflow-standalone bash` |
| **Conectar ao PostgreSQL:** | `docker compose exec postgres_erp psql -U postgres -d northwind` |

---

## üí° Conceitos Importantes para Avalia√ß√£o

### ETL vs ELT (Revis√£o R√°pida)

* **ETL:** Transforma√ß√£o pesada em *staging* (Ex: Pandas). Bom para dados estruturados, baixo custo de computa√ß√£o no destino.
* **ELT:** Transforma√ß√£o no Data Warehouse (Ex: dbt/SQL). Bom para Big Data, aproveita a escalabilidade do destino.

### Benef√≠cios da Automa√ß√£o

* **Consist√™ncia e Repetibilidade:** Resultados id√™nticos em toda execu√ß√£o.
* **Monitoramento:** Visibilidade instant√¢nea do status do pipeline e tratamento de erros.
* **Escalabilidade:** Capacidade de processar volumes crescentes de dados sem interven√ß√£o manual.

---

## ‚ñ∂Ô∏è Pr√≥ximos Passos (Discuss√£o)

Ao final do laborat√≥rio, discutiremos:
1.  **Revis√£o do C√≥digo:** Onde e como o Pandas foi usado para garantir a qualidade dos dados.
2.  **Monitoramento:** Como usar o Airflow UI para acompanhar e depurar.
3.  **DataOps:** A import√¢ncia de automa√ß√£o, testes e tratamento de erros para a opera√ß√£o de dados moderna.
4.  **Extens√µes:** Implementa√ß√£o de transforma√ß√µes mais complexas, testes de qualidade de dados e integra√ß√£o com ferramentas de BI.
