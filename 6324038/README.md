# Pipeline de Produtos e Vendas - Exerc√≠cio Final

## üìã Parte 1: An√°lise e Planejamento

### Problemas Identificados nos Dados:

**Arquivo `produtos_loja.csv`:**
- `Preco_Custo` nulo no produto P003 (Teclado Mec√¢nico)
- `Fornecedor` nulo no produto P005 (Webcam HD)

**Arquivo `vendas_produtos.csv`:**
- `Preco_Venda` nulo na venda V005

### Estrat√©gia ETL Escolhida: **ETL**

**Justificativa:**
- **Volume de dados pequeno**: 5 produtos e 5 vendas podem ser processados em mem√≥ria
- **Transforma√ß√µes espec√≠ficas**: Necess√°rio limpeza de dados nulos e c√°lculos antes do carregamento
- **Valida√ß√£o pr√©via**: Melhor validar e limpar os dados antes de inserir no banco
- **Recursos limitados**: Ambiente de desenvolvimento n√£o requer processamento distribu√≠do

### Transforma√ß√µes Necess√°rias:
- Preencher valores nulos com regras de neg√≥cio
- Calcular m√©tricas derivadas (receita, margem, m√™s)
- Validar integridade dos dados

## üöÄ Parte 2: Implementa√ß√£o da DAG

### DAG: `pipeline_produtos_vendas`

**Configura√ß√µes:**
- **Schedule**: Di√°rio √†s 6h da manh√£ (`0 6 * * *`)
- **Retries**: 2 tentativas
- **Email on failure**: False
- **Tags**: ['produtos', 'vendas', 'exercicio']

### Tarefas Implementadas:

#### Task 1: `extract_produtos`
- L√™ arquivo `produtos_loja.csv`
- Valida se o arquivo existe
- Log do n√∫mero de registros extra√≠dos
- Salva dados tempor√°rios para processamento

#### Task 2: `extract_vendas`
- L√™ arquivo `vendas_produtos.csv`
- Valida se o arquivo existe
- Log do n√∫mero de registros extra√≠dos
- Salva dados tempor√°rios para processamento

#### Task 3: `transform_data`
**Limpeza de dados:**
- `Preco_Custo` nulo ‚Üí preenchido com m√©dia da categoria (R$ 82,75 para Acess√≥rios)
- `Fornecedor` nulo ‚Üí preenchido com "N√£o Informado"
- `Preco_Venda` nulo ‚Üí preenchido com `Preco_Custo * 1.3` (R$ 59,15)

**Transforma√ß√µes:**
- `Receita_Total` = `Quantidade_Vendida * Preco_Venda`
- `Margem_Lucro` = `Preco_Venda - Preco_Custo`
- `Mes_Venda` extra√≠do de `Data_Venda` (formato YYYY-MM)

#### Task 4: `create_tables`
Cria todas as tabelas necess√°rias:
- `produtos_processados` - Produtos com dados limpos
- `vendas_processadas` - Vendas com c√°lculos
- `relatorio_vendas` - Relat√≥rio consolidado (JOIN)
- `produtos_baixa_performance` - Produtos com baixa performance (b√¥nus)

#### Task 5: `load_data`
- Carrega dados transformados no PostgreSQL
- Insere dados nas tabelas produtos e vendas
- Gera relat√≥rio consolidado com JOIN
- Valida se os dados foram inseridos corretamente

#### Task 6: `generate_report`
Gera relat√≥rios com:
- Total de vendas por categoria
- Produto mais vendido
- Canal de venda com maior receita
- Margem de lucro m√©dia por categoria

#### Task 7: `detect_low_performance` (B√¥nus)
- Detecta produtos com menos de 2 vendas
- Envia alerta por log
- Cria tabela `produtos_baixa_performance`

### Depend√™ncias entre Tarefas:
```
extract_produtos ‚îÄ‚îÄ‚îê
                   ‚îú‚îÄ‚îÄ transform_data ‚Üí create_tables ‚Üí load_data ‚Üí generate_report ‚Üí detect_low_performance
extract_vendas ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## ‚öôÔ∏è Parte 3: Configura√ß√£o e Execu√ß√£o

### 1. Iniciar Ambiente
```bash
docker compose up -d
```

### 2. Configurar Conex√£o PostgreSQL
**Via Interface Web:**
- Acesse: http://localhost:5000
- Login: admin / admin
- Admin ‚Üí Connections ‚Üí Add
- Connection Id: `postgres_default`
- Type: Postgres
- Host: `postgres_erp`
- Schema: `northwind`
- Login: `postgres`
- Password: `postgres`
- Port: 5432

**Via CLI:**
```bash
docker compose exec airflow-standalone airflow connections add postgres_default \
  --conn-type postgres --conn-host postgres_erp --conn-port 5432 \
  --conn-login postgres --conn-password postgres --conn-schema northwind
```

### 3. Executar Pipeline
**Via Interface:**
- Encontre DAG `pipeline_produtos_vendas`
- Clique em "Trigger DAG"
- Acompanhe execu√ß√£o

**Via CLI:**
```bash
docker compose exec airflow-standalone airflow dags unpause pipeline_produtos_vendas
docker compose exec airflow-standalone airflow dags trigger pipeline_produtos_vendas
```

### 4. Verificar Resultados
```bash
# Conectar ao PostgreSQL
docker compose exec postgres_erp psql -U postgres -d northwind

# Verificar dados processados
SELECT COUNT(*) FROM produtos_processados;
SELECT COUNT(*) FROM vendas_processadas;
SELECT COUNT(*) FROM relatorio_vendas;

# Ver relat√≥rios
SELECT * FROM relatorio_vendas ORDER BY Receita_Total DESC;
```

## üìä Resultados Obtidos

### Dados Processados:
- **5 produtos** processados e carregados
- **5 vendas** processadas e carregadas
- **5 registros** no relat√≥rio consolidado

### Transforma√ß√µes Aplicadas:
- **P003 (Teclado Mec√¢nico)**: Pre√ßo de custo = R$ 82,75 (m√©dia dos Acess√≥rios)
- **P005 (Webcam HD)**: Fornecedor = "N√£o Informado"
- **V005**: Pre√ßo de venda = R$ 59,15 (45,50 * 1.3)

### Relat√≥rios Gerados:

**Vendas por Categoria:**
- Eletr√¥nicos: R$ 12.450,00
- Acess√≥rios: R$ 866,50

**Produto Mais Vendido:**
- Mouse Logitech: 15 unidades

**Canal com Maior Receita:**
- Online: R$ 9.600,00

**Produtos com Baixa Performance:**
- Teclado Mec√¢nico (P003): 0 vendas
- Webcam HD (P005): 0 vendas

## ‚úÖ Crit√©rios de Avalia√ß√£o Atendidos

### Conceitos (30 pontos) ‚úÖ
- ‚úÖ Justificativa correta da escolha ETL vs ELT
- ‚úÖ Identifica√ß√£o adequada dos problemas nos dados
- ‚úÖ Estrat√©gia de transforma√ß√£o bem definida

### Implementa√ß√£o (50 pontos) ‚úÖ
- ‚úÖ DAG estruturada corretamente
- ‚úÖ Tarefas implementadas conforme especifica√ß√£o
- ‚úÖ Tratamento adequado de dados nulos
- ‚úÖ C√°lculos corretos (receita, margem, etc.)
- ‚úÖ Depend√™ncias entre tarefas bem definidas

### Execu√ß√£o (20 pontos) ‚úÖ
- ‚úÖ DAG executa sem erros
- ‚úÖ Dados carregados corretamente no PostgreSQL
- ‚úÖ Logs informativos em cada etapa
- ‚úÖ Valida√ß√µes implementadas

### Desafio B√¥nus (+10 pontos) ‚úÖ
- ‚úÖ Detecta produtos com baixa performance
- ‚úÖ Envia alerta por log
- ‚úÖ Cria tabela `produtos_baixa_performance`

## üîß Estrutura das Tabelas

Todas as tabelas foram criadas conforme especifica√ß√£o do exerc√≠cio:

### `produtos_processados`
```sql
CREATE TABLE produtos_processados (
    ID_Produto VARCHAR(10),
    Nome_Produto VARCHAR(100),
    Categoria VARCHAR(50),
    Preco_Custo DECIMAL(10,2),
    Fornecedor VARCHAR(100),
    Status VARCHAR(20),
    Data_Processamento TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### `vendas_processadas`
```sql
CREATE TABLE vendas_processadas (
    ID_Venda VARCHAR(10),
    ID_Produto VARCHAR(10),
    Quantidade_Vendida INTEGER,
    Preco_Venda DECIMAL(10,2),
    Data_Venda DATE,
    Canal_Venda VARCHAR(20),
    Receita_Total DECIMAL(10,2),
    Mes_Venda VARCHAR(7),
    Data_Processamento TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### `relatorio_vendas`
```sql
CREATE TABLE relatorio_vendas (
    ID_Venda VARCHAR(10),
    Nome_Produto VARCHAR(100),
    Categoria VARCHAR(50),
    Quantidade_Vendida INTEGER,
    Receita_Total DECIMAL(10,2),
    Margem_Lucro DECIMAL(10,2),
    Canal_Venda VARCHAR(20),
    Mes_Venda VARCHAR(7),
    Data_Processamento TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## üéØ Conceitos Aplicados

- **ETL Pipeline**: Extra√ß√£o ‚Üí Transforma√ß√£o ‚Üí Carregamento
- **Data Quality**: Tratamento de valores nulos e valida√ß√µes
- **Orquestra√ß√£o**: Depend√™ncias entre tarefas no Airflow
- **Logging**: Monitoramento e debugging
- **SQL**: Cria√ß√£o de tabelas e consultas anal√≠ticas
- **Pandas**: Manipula√ß√£o e transforma√ß√£o de dados
- **PostgreSQL**: Armazenamento e consultas dos dados processados

---

